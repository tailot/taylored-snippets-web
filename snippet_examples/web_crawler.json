[
  {
    "id": 0,
    "type": "compute",
    "value": "#!/usr/bin/env bash\n\n# --- CONFIGURATION ---\n# The URL to start crawling from.\nSTART_URL=\"https://www......../\"\n\n# The directory where all files will be saved (will be created if it does not exist).\nOUTPUT_DIR=\"/crawler_results\"\n\n# Limit crawling to the starting domain? (true/false)\n# If 'true', it will not follow links to external sites.\nLIMIT_TO_DOMAIN=true\n\n# Delay in seconds between requests to avoid overloading the server.\nREQUEST_DELAY=1\n# --- END CONFIGURATION ---\n\n# --- PREPARATION ---\n# Create the output directory if it doesn't exist. The -p flag prevents errors if it's already there.\nmkdir -p \"$OUTPUT_DIR\"\n\n# Define the full paths for the working files\nQUEUE_FILE=\"$OUTPUT_DIR/queue.txt\"\nVISITED_FILE=\"$OUTPUT_DIR/visited.txt\"\nCOLLECTED_FILE=\"$OUTPUT_DIR/collected.txt\"\n\n# 'touch' creates the files if they don't exist.\ntouch \"$QUEUE_FILE\"\ntouch \"$VISITED_FILE\"\ntouch \"$COLLECTED_FILE\"\n\n# Add the starting URL to the queue\necho \"$START_URL\" > \"$QUEUE_FILE\"\n\n# Extract the base domain to limit the search (e.g., https://www.alpinelinux.org)\nBASE_DOMAIN=$(echo \"$START_URL\" | grep -o 'https?://[^/]*')\n\n# Main loop: continues as long as there are URLs in the queue\nwhile [ -s \"$QUEUE_FILE\" ]; do\n    # Get the first URL from the queue\n    CURRENT_URL=$(head -n 1 \"$QUEUE_FILE\")\n    # Remove the URL we just took from the queue\n    sed -i '1d' \"$QUEUE_FILE\"\n\n    # Check if we have already visited this URL\n    if grep -q -x \"$CURRENT_URL\" \"$VISITED_FILE\"; then\n        echo \"  Already visited: $CURRENT_URL\"\n        continue # Skip to the next URL in the loop\n    fi\n\n    echo \"Visiting: $CURRENT_URL\"\n\n    # Add the current URL to the visited list and the results\n    echo \"$CURRENT_URL\" >> \"$VISITED_FILE\"\n    echo \"$CURRENT_URL\" >> \"$COLLECTED_FILE\"\n\n    # Download the page and find new links\n    NEW_LINKS=$(curl -s -L -m 10 \"$CURRENT_URL\" | grep -Eo 'href=\"([^\"]+)\"' | cut -d'\"' -f2)\n\n    for link in $NEW_LINKS; do\n        # Convert relative links to absolute links\n        case \"$link\" in\n          \\#* | mailto:* | javascript:*)\n            continue ;; # Ignore anchors, email links, or javascript calls\n          http*)\n            abs_link=\"$link\" ;; # It's already an absolute link\n          //*)\n            abs_link=\"https:$link\" ;; # Protocol-relative link\n          /*)\n            abs_link=\"$BASE_DOMAIN$link\" ;; # Root-relative link\n          *)\n            abs_link=\"$CURRENT_URL$link\" ;; # Page-relative link\n        esac\n\n        # Clean up the URL by removing any trailing slash to avoid duplicates\n        abs_link=$(echo \"$abs_link\" | sed 's|/$||')\n\n        # If LIMIT_TO_DOMAIN is true, check that the link belongs to the base domain\n        if [ \"$LIMIT_TO_DOMAIN\" = true ] && ! [[ \"$abs_link\" == \"$BASE_DOMAIN\"* ]]; then\n            continue # Skip external links\n        fi\n\n        # Add the new link to the queue only if it hasn't been visited yet\n        if ! grep -q -x \"$abs_link\" \"$VISITED_FILE\"; then\n            echo \"$abs_link\" >> \"$QUEUE_FILE\"\n        fi\n    done\n\n    # Wait before the next request\n    sleep \"$REQUEST_DELAY\"\ndone\n\n# Clean up the final results by sorting and removing duplicates\nsort -u \"$COLLECTED_FILE\" -o \"$COLLECTED_FILE\"\n\necho \"\"\necho \"âœ… Crawling complete!\"\necho \"Found $(wc -l < \"$COLLECTED_FILE\") unique links. Results saved in: $COLLECTED_FILE\"\n\n# Remove temporary files\nrm \"$QUEUE_FILE\" \"$VISITED_FILE\""
  }
]